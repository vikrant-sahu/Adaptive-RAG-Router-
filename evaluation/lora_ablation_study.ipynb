{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94db12ad",
   "metadata": {},
   "source": [
    "# LoRA Ablation Study Notebook\n",
    "\n",
    "Comprehensive comparison of different LoRA ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909e5c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from data.data_loader import CLINC150DataLoader\n",
    "from models.adaptive_router import create_router_model\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# Set plot style (based on source initialization)\n",
    "plt.style.use('default')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a18cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRAAblationStudy:\n",
    "    \"\"\"Comprehensive LoRA rank ablation study\"\"\"\n",
    "\n",
    "    def __init__(self, output_dir: str = \"./ablation_results\"):\n",
    "        self.output_dir = output_dir\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        # LoRA ranks to test [5]\n",
    "        self.lora_ranks = [2-4]\n",
    "        self.models_to_test = [\"distilbert\", \"roberta\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72354cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _test_single_config(self, model_type: str, lora_rank: int, num_samples: int) -> dict:\n",
    "    \"\"\"Test single model configuration\"\"\"\n",
    "    \n",
    "    random.seed(42)\n",
    "\n",
    "    # Initialize model with specific LoRA rank [2]\n",
    "    model = create_router_model(model_type=model_type, lora_rank=lora_rank)\n",
    "\n",
    "    # Load test data [2]\n",
    "    data_loader = CLINC150DataLoader(max_length=128)\n",
    "    test_dataset = data_loader.get_processed_dataset(\"test\")\n",
    "\n",
    "    # Use subset for faster evaluation [2]\n",
    "    indices = random.sample(range(len(test_dataset)), min(num_samples, len(test_dataset)))\n",
    "    subset = test_dataset.select(indices)\n",
    "\n",
    "    # Predict [6]\n",
    "    texts = [data_loader.load_dataset(\"test\")[i][\"text\"] for i in indices]\n",
    "    true_labels = [subset[i][\"labels\"] for i in range(len(subset))]\n",
    "    \n",
    "    start_time = time.time()\n",
    "    predictions = model.predict(texts)\n",
    "    inference_time = time.time() - start_time\n",
    "\n",
    "    # Calculate metrics [6]\n",
    "    accuracy = accuracy_score(true_labels, predictions[\"predictions\"])\n",
    "    f1 = f1_score(true_labels, predictions[\"predictions\"], average=\"weighted\")\n",
    "\n",
    "    # Count trainable parameters [6, 7]\n",
    "    trainable_params = sum(p.numel() for p in model.model.parameters() if p.requires_grad)\n",
    "    total_params = sum(p.numel() for p in model.model.parameters())\n",
    "\n",
    "    return { \n",
    "        \"model_type\": model_type,\n",
    "        \"lora_rank\": lora_rank,\n",
    "        \"accuracy\": accuracy,\n",
    "        \"f1_score\": f1,\n",
    "        \"inference_time_ms\": (inference_time / len(texts)) * 1000,\n",
    "        \"trainable_params\": trainable_params,\n",
    "        \"total_params\": total_params,\n",
    "        \"parameter_efficiency\": trainable_params / total_params,\n",
    "        \"samples_tested\": len(texts)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186a2190",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ablation(self, num_samples: int = 1000) -> pd.DataFrame:\n",
    "    \"\"\"Run ablation study across models and LoRA ranks\"\"\" [5]\n",
    "\n",
    "    results = []\n",
    "    for model_type in self.models_to_test:\n",
    "        print(f\"\\nðŸ”¬ Testing {model_type} with different LoRA ranks...\")\n",
    "        for rank in tqdm(self.lora_ranks, desc=f\"{model_type} ranks\"):\n",
    "            try:\n",
    "                result = self._test_single_config(model_type, rank, num_samples)\n",
    "                results.append(result) [8]\n",
    "            except Exception as e:\n",
    "                print(f\"Error testing {model_type} rank {rank}: {e}\")\n",
    "                continue\n",
    "\n",
    "    # Create results dataframe [8]\n",
    "    df = pd.DataFrame(results)\n",
    "\n",
    "    # Save results [8]\n",
    "    df.to_csv(os.path.join(self.output_dir, \"lora_ablation_results.csv\"), index=False)\n",
    "\n",
    "    # Generate plots and analysis [8, 9]\n",
    "    self._generate_plots(df)\n",
    "    self._save_detailed_analysis(df)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af37136",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _generate_plots(self, df: pd.DataFrame):\n",
    "    \"\"\"Generate comprehensive ablation study plots\"\"\" [7]\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    fig.suptitle('LoRA Rank Ablation Study', fontsize=16, fontweight='bold') [10]\n",
    "\n",
    "    # Plot 1: Accuracy vs LoRA Rank\n",
    "    ax = axes\n",
    "    for model_type in df['model_type'].unique():\n",
    "        model_data = df[df['model_type'] == model_type]\n",
    "        ax.plot(model_data['lora_rank'], model_data['accuracy'],\n",
    "                        marker='o', label=model_type, linewidth=2) [10]\n",
    "    ax.set_xlabel('LoRA Rank')\n",
    "    ax.set_ylabel('Accuracy')\n",
    "    ax.set_title('Accuracy vs LoRA Rank')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "    # Plot 2: F1 Score vs LoRA Rank\n",
    "    ax = axes[1]\n",
    "    for model_type in df['model_type'].unique():\n",
    "        model_data = df[df['model_type'] == model_type]\n",
    "        ax.plot(model_data['lora_rank'], model_data['f1_score'],\n",
    "                        marker='s', label=model_type, linewidth=2) [3]\n",
    "    ax.set_xlabel('LoRA Rank')\n",
    "    ax.set_ylabel('F1 Score')\n",
    "    ax.set_title('F1 Score vs LoRA Rank')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "    # Plot 3: Inference Time vs LoRA Rank\n",
    "    ax = axes[5]\n",
    "    for model_type in df['model_type'].unique():\n",
    "        model_data = df[df['model_type'] == model_type]\n",
    "        ax.plot(model_data['lora_rank'], model_data['inference_time_ms'],\n",
    "                        marker='^', label=model_type, linewidth=2) [11]\n",
    "    ax.set_xlabel('LoRA Rank')\n",
    "    ax.set_ylabel('Inference Time (ms)')\n",
    "    ax.set_title('Inference Time vs LoRA Rank')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "    # Plot 4: Parameter Efficiency\n",
    "    ax = axes[1]\n",
    "    for model_type in df['model_type'].unique():\n",
    "        model_data = df[df['model_type'] == model_type]\n",
    "        ax.plot(model_data['lora_rank'], model_data['parameter_efficiency'] * 100,\n",
    "                        marker='d', label=model_type, linewidth=2) [12]\n",
    "    ax.set_xlabel('LoRA Rank')\n",
    "    ax.set_ylabel('Parameter Efficiency (%)')\n",
    "    ax.set_title('Parameter Efficiency vs LoRA Rank')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "    # Plot 5: Trainable Parameters\n",
    "    ax = axes[1, 1]\n",
    "    for model_type in df['model_type'].unique():\n",
    "        model_data = df[df['model_type'] == model_type]\n",
    "        ax.plot(model_data['lora_rank'], model_data['trainable_params'],\n",
    "                        marker='v', label=model_type, linewidth=2) [13]\n",
    "    ax.set_xlabel('LoRA Rank')\n",
    "    ax.set_ylabel('Trainable Parameters')\n",
    "    ax.set_title('Trainable Parameters vs LoRA Rank')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "    # Plot 6: Performance vs Efficiency Trade-off\n",
    "    ax = axes[1, 5]\n",
    "    for model_type in df['model_type'].unique():\n",
    "        model_data = df[df['model_type'] == model_type]\n",
    "        ax.scatter(model_data['inference_time_ms'], model_data['accuracy'],\n",
    "                            s=100, alpha=0.7, label=model_type) [9]\n",
    "        # Add rank annotations\n",
    "        for _, row in model_data.iterrows():\n",
    "            ax.annotate(f\"r={row['lora_rank']}\",\n",
    "                                        (row['inference_time_ms'], row['accuracy']),\n",
    "                                        textcoords=\"offset points\", xytext=(5,5),\n",
    "                                        ha='left', fontsize=8)\n",
    "    ax.set_xlabel('Inference Time (ms)')\n",
    "    ax.set_ylabel('Accuracy')\n",
    "    ax.set_title('Performance vs Efficiency Trade-off')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(self.output_dir, 'lora_ablation_study.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff51d7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _generate_recommendations(self, df: pd.DataFrame) -> list:\n",
    "    \"\"\"Generate practical recommendations from results\"\"\" [14]\n",
    "\n",
    "    recommendations = []\n",
    "    # Find best configurations\n",
    "    best_accuracy = df.loc[df['accuracy'].idxmax()]\n",
    "    best_efficiency = df.loc[df['parameter_efficiency'].idxmax()]\n",
    "    best_balanced = df.loc[(df['accuracy'] > df['accuracy'].quantile(0.8)) &\n",
    "                            (df['inference_time_ms'] < df['inference_time_ms'].quantile(0.5))].iloc [14]\n",
    "\n",
    "    recommendations.extend([\n",
    "        f\"Best Accuracy: {best_accuracy['model_type']} with r={best_accuracy['lora_rank']} \"\n",
    "        f\"(Accuracy: {best_accuracy['accuracy']:.4f})\", [15]\n",
    "        f\"Most Efficient: {best_efficiency['model_type']} with r={best_efficiency['lora_rank']} \"\n",
    "        f\"(Efficiency: {best_efficiency['parameter_efficiency']*100:.1f}%)\", [15]\n",
    "        f\"Best Balanced: {best_balanced['model_type']} with r={best_balanced['lora_rank']} \"\n",
    "        f\"(Accuracy: {best_balanced['accuracy']:.4f}, Time: {best_balanced['inference_time_ms']:.2f}ms)\", [15]\n",
    "        \"Optimal LoRA Rank: r=16 provides best balance of accuracy and efficiency for most models\" [4]\n",
    "    ])\n",
    "    return recommendations\n",
    "\n",
    "def _save_detailed_analysis(self, df: pd.DataFrame):\n",
    "    \"\"\"Save detailed analysis of results\"\"\" [16]\n",
    "    \n",
    "    analysis = {\n",
    "        \"summary\": {\n",
    "            \"best_overall_accuracy\": df['accuracy'].max(),\n",
    "            \"best_config\": df.loc[df['accuracy'].idxmax()].to_dict(),\n",
    "            \"fastest_inference\": df.loc[df['inference_time_ms'].idxmin()].to_dict(),\n",
    "            \"most_efficient\": df.loc[df['parameter_efficiency'].idxmax()].to_dict()\n",
    "        },\n",
    "        \"recommendations\": self._generate_recommendations(df)\n",
    "    }\n",
    "\n",
    "    with open(os.path.join(self.output_dir, \"detailed_analysis.json\"), \"w\") as f:\n",
    "        json.dump(analysis, f, indent=2)\n",
    "\n",
    "    print(\"\\nðŸ“‹ Ablation Study Recommendations:\") [14]\n",
    "    print(\"=\" * 50)\n",
    "    for rec in analysis[\"recommendations\"]:\n",
    "        print(f\"â€¢ {rec}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4b8868",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execution Block\n",
    "\n",
    "print(\"ðŸ”¬ Starting LoRA Rank Ablation Study\") [4]\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Instantiate and run the study\n",
    "study = LoRAAblationStudy()\n",
    "# Using num_samples=2000 as specified in the original main function [4]\n",
    "results_df = study.run_ablation(num_samples=2000)\n",
    "\n",
    "print(\"\\nðŸ“Š Ablation Study Completed!\") [4]\n",
    "print(\"Results saved to ./ablation_results/\") [4]\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "results_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
